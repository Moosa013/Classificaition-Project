{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dac625d-c435-4927-9256-38eca9951d5d",
   "metadata": {},
   "source": [
    "# NLP & Classification Group Project\n",
    "\n",
    "### Project Title: News Article Category Predictions\n",
    "#### Done By: Amogelang Mogwane, Chris Phillip, Moosa Molibeli & Tiisetso Gabaza\n",
    "\n",
    "© ExploreAI 2024\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a7af2-af30-4fdc-8848-2aa468b291a7",
   "metadata": {},
   "source": [
    "![NewsPaper](newspaper1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c69e3e-0dbb-4735-8314-5082e5ebbaf2",
   "metadata": {},
   "source": [
    "<a id=\"cont\"></a>\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Background Context</a>\n",
    "\n",
    "<a href=#two>2. Importing Packages</a>\n",
    "\n",
    "<a href=#three>3. Data Collection and Description</a>\n",
    "\n",
    "<a href=#four>4. Loading Data </a>\n",
    "\n",
    "<a href=#five>5. Data Cleaning and PreProcessing</a>\n",
    "\n",
    "<a href=#six>6. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#seven>7. Modeling</a>\n",
    "\n",
    "<a href=#eight>8. Evaluation</a>\n",
    "\n",
    "<a href=#nine>9. Final Model</a>\n",
    "\n",
    "<a href=#ten>10. Streamlit Deployment</a>\n",
    "\n",
    "<a href=#eleven>11. Conclusion and Future Work</a>\n",
    "\n",
    "<a href=#twelve>12. References</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11960c91-6da9-41fc-a69e-ae34a39ecae0",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"one\"></a>\n",
    "## Background Context\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e52f5ab-2e2a-464a-9678-402ac3a4e07e",
   "metadata": {},
   "source": [
    "From the inception of printed newspapers, every article has been assigned to a specific section. While many aspects of the newspaper industry have evolved—ranging from ink and paper types to distribution methods—the practice of categorizing news has persisted across generations, extending into digital formats. Newspaper articles cover a broad spectrum of topics, including politics, sports, and entertainment. Historically, categorization was primarily a manual process, but advancements in technology now allow for automated classification with minimal effort.\n",
    "\n",
    "This project aims to design and develop an application that predicts the categories of news articles intended for publication. By utilizing classification algorithms, we will analyze the content of articles to determine their respective genres. The proposed algorithm will not only classify existing topics but also adapt to new topics as they emerge in the world. While the algorithm is extendable to multiple languages, this paper will primarily focus on English.\n",
    "\n",
    "In the realm of news article classification, multi-label text classification poses a significant challenge. Our objective is to assign one or more category labels to each article. For each category, classifiers will provide binary responses—either \"yes\" or \"no\"—indicating whether a specific category applies to the given test data. This approach utilizes binary classifiers, and we will implement several standard algorithms, including Naive Bayes, Support Vector Machines, and Logistic Regression, which are commonly used for binary classification. We will evaluate these approaches and select the best model based on predetermined parameters for predicting news categories.\n",
    "\n",
    "A robust category classification algorithm for news articles must achieve high acuuracy while remaining easily updatable. Given the continuous evolution of news topics and events, the ability to seamlessly add new categories to the classifier is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6455690-f3a6-4147-82ca-48ccc53c7673",
   "metadata": {},
   "source": [
    "![News Articles](newspaper2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53369b0b-4e3c-4d5e-875e-4652a29a1674",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"two\"></a>\n",
    "## Importing Packages\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14941b06-3170-44a1-9646-1b0a50021b35",
   "metadata": {},
   "source": [
    "**Please Note:**\n",
    "*The below are all the libraries I believe I will need for this project. This list will be adjusted as needed throughout the project*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8633897-6f34-41b3-a513-0679e3878971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.combine import SMOTEENN \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "import joblib \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edf9cc41-79e1-4f12-9816-b708135ff283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please run this code if you do not have wordcloud installed in your environment\n",
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11a1a5d3-e6cf-4b10-b91f-f80eab275c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please run this code if you do not have imbalanced-learn installed in your environment\n",
    "#pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41b5e9c4-2ee9-461b-9b36-3f616659ddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please run this code if you do not have xgboost installed in your environment\n",
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4ee3648-618f-4a70-b2f4-7d10398cf508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please run this code if you do not have xgboost installed in your environment\n",
    "#!pip install streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721dbba7-a30b-47de-99e6-08fe50e643a0",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"three\"></a>\n",
    "## Data Collection and Description\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceea719-06a5-4e07-ac69-a5177d2b28a0",
   "metadata": {},
   "source": [
    "This project utilizes a news article dataset containing articles categorized into five distinct groups: Sports, Business, Entertainment, Education, and Technology. The dataset has already been randomly partitioned into training and testing sets, with the challenge of developing a classification model to predict the category of each news article.\n",
    "\n",
    "The training dataset comprises 5,520 records across five columns, while the testing dataset consists of 2,000 observations with the same column structure. The columns included in the dataset are: 'index', 'headlines', 'description', 'content', 'url', and 'category'. The target variable for prediction is the 'category' column. Other columns will either be discarded (e.g., the 'url' column) or combined to create a single content column for the articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71ff5c2-f6da-4e03-9d7b-fe3c9ff150ab",
   "metadata": {},
   "source": [
    "Below is a description of the dataset features:\n",
    "\n",
    "**Dataset Features:**\n",
    "| **Column**                                                                                  | **Description**              \n",
    "|---------------------------------------------------------------------------------------------|--------------------   \n",
    "| Headlines   | \tThe headline or title of the news article.\n",
    "| Description | A brief summary or description of the news article.\n",
    "| Content | The full text content of the news article.\n",
    "| URL | The URL link to the original source of the news article.\n",
    "| Category | The category or topic of the news article (e.g., business, education, entertainment, sports, technology)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd33686-53ba-4c58-aa5d-cbb32283ccdf",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"four\"></a>\n",
    "## Loading Data\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0692a69c-b68b-4108-a55b-0002fbad4c39",
   "metadata": {},
   "source": [
    "As the datasets have been provided in csv formats, I will upload both of the train.csv and test.csv datasets. The approach taken in this workbook is that we will follow the same steps to preprocess both datasets in order to maintain alignment when validation of our model is required in the evaluation step of this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fed44c16-cf6a-4e7b-bb2a-2ecbc6504f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading our training and our testing data sets\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adf5b899-ed4f-420c-bb39-c90f9464d462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RBI revises definition of politically-exposed ...</td>\n",
       "      <td>The central bank has also asked chairpersons a...</td>\n",
       "      <td>The Reserve Bank of India (RBI) has changed th...</td>\n",
       "      <td>https://indianexpress.com/article/business/ban...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDTV Q2 net profit falls 57.4% to Rs 5.55 cror...</td>\n",
       "      <td>NDTV's consolidated revenue from operations wa...</td>\n",
       "      <td>Broadcaster New Delhi Television Ltd on Monday...</td>\n",
       "      <td>https://indianexpress.com/article/business/com...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akasa Air ‘well capitalised’, can grow much fa...</td>\n",
       "      <td>The initial share sale will be open for public...</td>\n",
       "      <td>Homegrown server maker Netweb Technologies Ind...</td>\n",
       "      <td>https://indianexpress.com/article/business/mar...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India’s current account deficit declines sharp...</td>\n",
       "      <td>The current account deficit (CAD) was 3.8 per ...</td>\n",
       "      <td>India’s current account deficit declined sharp...</td>\n",
       "      <td>https://indianexpress.com/article/business/eco...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>States borrowing cost soars to 7.68%, highest ...</td>\n",
       "      <td>The prices shot up reflecting the overall high...</td>\n",
       "      <td>States have been forced to pay through their n...</td>\n",
       "      <td>https://indianexpress.com/article/business/eco...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0  RBI revises definition of politically-exposed ...   \n",
       "1  NDTV Q2 net profit falls 57.4% to Rs 5.55 cror...   \n",
       "2  Akasa Air ‘well capitalised’, can grow much fa...   \n",
       "3  India’s current account deficit declines sharp...   \n",
       "4  States borrowing cost soars to 7.68%, highest ...   \n",
       "\n",
       "                                         description  \\\n",
       "0  The central bank has also asked chairpersons a...   \n",
       "1  NDTV's consolidated revenue from operations wa...   \n",
       "2  The initial share sale will be open for public...   \n",
       "3  The current account deficit (CAD) was 3.8 per ...   \n",
       "4  The prices shot up reflecting the overall high...   \n",
       "\n",
       "                                             content  \\\n",
       "0  The Reserve Bank of India (RBI) has changed th...   \n",
       "1  Broadcaster New Delhi Television Ltd on Monday...   \n",
       "2  Homegrown server maker Netweb Technologies Ind...   \n",
       "3  India’s current account deficit declined sharp...   \n",
       "4  States have been forced to pay through their n...   \n",
       "\n",
       "                                                 url  category  \n",
       "0  https://indianexpress.com/article/business/ban...  business  \n",
       "1  https://indianexpress.com/article/business/com...  business  \n",
       "2  https://indianexpress.com/article/business/mar...  business  \n",
       "3  https://indianexpress.com/article/business/eco...  business  \n",
       "4  https://indianexpress.com/article/business/eco...  business  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting a sense of the training data from the first 5 observations\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73e2f43-2e37-4fed-9b9c-3e0f4c47e7ea",
   "metadata": {},
   "source": [
    "Above we printed the first five rows of the dataset to ensure that it has loaded correctly into our notebook. \n",
    "\n",
    "Below we will check the shape of both datasets to ensure that they do have 5,520 and 2,000 observation as we anticipate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f471442-70a2-4f0a-bcbd-441f8de6e180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5520, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# understanding the structure of the training dataset\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd8a13c6-fa81-4f2d-b0e0-a625536f8214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# understanding the structure of the testing dataset\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea87210-63fe-49ab-95c2-2abad0bc9055",
   "metadata": {},
   "source": [
    "Observation:\n",
    "\n",
    "From observation, I can indeed confirm that both datasets have loaded correctly and completely into the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee64b633-fefd-4d86-9baf-45086fef46b3",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"five\"></a>\n",
    "## Data Cleaning and PreProcessing\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f4f11d-50e4-46e0-af18-b418b7783424",
   "metadata": {},
   "source": [
    "Data cleaning is an essential first step in any data-driven project, ensuring the dataset is accurate, consistent, and ready for analysis. For our news article classification project, this involves several processes to improve data quality.\n",
    "\n",
    "We will start by creating a copy of the training dataset. This way, any major changes can be made while preserving the original for reference.\n",
    "All the steps we do to our training dataset, we will also apply to our testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b089e79e-8a2e-4885-82c5-4e73439d21fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a copy of the training dataframe\n",
    "train_df_copy = train_df.copy()\n",
    "test_df_copy = test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db618f7a-f576-444e-948b-690ea72737da",
   "metadata": {},
   "source": [
    "Next, we'll address any missing or incomplete data. Missing values can distort results and lead to inaccurate conclusions, so it's crucial to identify and handle these gaps through imputation, removal, or replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0615c75-a4ab-4dd2-8c80-53407e6814cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headlines      0\n",
       "description    0\n",
       "content        0\n",
       "url            0\n",
       "category       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for missing values\n",
    "train_df_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c05f6ff7-1344-421a-96df-e9f4e0624697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headlines      0\n",
       "description    0\n",
       "content        0\n",
       "url            0\n",
       "category       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for missing values\n",
    "test_df_copy.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2790467b-817f-4370-94a7-698daa17a668",
   "metadata": {},
   "source": [
    "Since there are no missing values, we can confidently move on to the next phase of the project.\n",
    "\n",
    "Next, we will address any duplicate records to avoid bias in model training. Ensuring each article is unique is essential for accurately training the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c497e7c-0f18-43b3-b111-80ac243b7e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5520, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping any duplicates that might exist\n",
    "train_df_copy.drop_duplicates(inplace=True)\n",
    "\n",
    "train_df_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1454f386-1f6c-4ff4-81f5-034b3564d328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping any duplicates that might exist\n",
    "test_df_copy.drop_duplicates(inplace=True)\n",
    "\n",
    "test_df_copy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c3f7b0-9cf2-49d7-bb96-dbe3a1b18dbe",
   "metadata": {},
   "source": [
    "The structure of the training dataset remains intact, confirming there are no duplicates. We can proceed with confidence, knowing our dataset is unbiased for modeling.\n",
    "\n",
    "Next, we will remove the URL column, as it does not add value to our news category predictions and is considered redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c30c736-3376-4398-98d0-e454c550219c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RBI revises definition of politically-exposed ...</td>\n",
       "      <td>The central bank has also asked chairpersons a...</td>\n",
       "      <td>The Reserve Bank of India (RBI) has changed th...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDTV Q2 net profit falls 57.4% to Rs 5.55 cror...</td>\n",
       "      <td>NDTV's consolidated revenue from operations wa...</td>\n",
       "      <td>Broadcaster New Delhi Television Ltd on Monday...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akasa Air ‘well capitalised’, can grow much fa...</td>\n",
       "      <td>The initial share sale will be open for public...</td>\n",
       "      <td>Homegrown server maker Netweb Technologies Ind...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India’s current account deficit declines sharp...</td>\n",
       "      <td>The current account deficit (CAD) was 3.8 per ...</td>\n",
       "      <td>India’s current account deficit declined sharp...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>States borrowing cost soars to 7.68%, highest ...</td>\n",
       "      <td>The prices shot up reflecting the overall high...</td>\n",
       "      <td>States have been forced to pay through their n...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0  RBI revises definition of politically-exposed ...   \n",
       "1  NDTV Q2 net profit falls 57.4% to Rs 5.55 cror...   \n",
       "2  Akasa Air ‘well capitalised’, can grow much fa...   \n",
       "3  India’s current account deficit declines sharp...   \n",
       "4  States borrowing cost soars to 7.68%, highest ...   \n",
       "\n",
       "                                         description  \\\n",
       "0  The central bank has also asked chairpersons a...   \n",
       "1  NDTV's consolidated revenue from operations wa...   \n",
       "2  The initial share sale will be open for public...   \n",
       "3  The current account deficit (CAD) was 3.8 per ...   \n",
       "4  The prices shot up reflecting the overall high...   \n",
       "\n",
       "                                             content  category  \n",
       "0  The Reserve Bank of India (RBI) has changed th...  business  \n",
       "1  Broadcaster New Delhi Television Ltd on Monday...  business  \n",
       "2  Homegrown server maker Netweb Technologies Ind...  business  \n",
       "3  India’s current account deficit declined sharp...  business  \n",
       "4  States have been forced to pay through their n...  business  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing redundant url column \n",
    "train_df_copy.drop(columns=\"url\",inplace=True)\n",
    "test_df_copy.drop(columns=\"url\",inplace=True)\n",
    "\n",
    "train_df_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3e3548-b70f-402d-af68-369c3b5fb639",
   "metadata": {},
   "source": [
    "Lastly, we will standardize the text data by converting the text in the three feature columns to lowercase for consistency, removing punctuation, eliminating stop words that add little meaning, and lemmatizing the dataset. These steps will streamline the data, making it easier for classification algorithms to process effectively. Lemmatization will reduce words to their base forms, enhancing classification across all observations.\n",
    "\n",
    "We will define a function to perform these final cleanup steps on dataframes. Recall that what we do to the training dataset will also be applied to the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4d6f91b-c512-4edc-ae5a-f6932586c89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# text cleaning \n",
    "def clean_text(text):\n",
    "    \n",
    "    text = re.sub(r'\\W', ' ', text)  # removing special characters\n",
    "    text = text.lower()  # lowercasing the text\n",
    "    tokens = word_tokenize(text)  # tokenizing the text\n",
    "    \n",
    "    # removing stop words and lemmatizing to reduce the words to base words\n",
    "    lemmatizer = WordNetLemmatizer()  # initialization\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_tokens = [\n",
    "        lemmatizer.lemmatize(token) for token in tokens if token not in stop_words\n",
    "    ]\n",
    "\n",
    "    # joining tokens back to string\n",
    "    return ' '.join(cleaned_tokens)\n",
    "\n",
    "columns_to_clean = ['headlines', 'description', 'content']\n",
    "\n",
    "# applying the clean_text function to the relevant columns in the train and test datasets\n",
    "train_df_copy[columns_to_clean] = train_df_copy[columns_to_clean].applymap(clean_text)\n",
    "test_df_copy[columns_to_clean] = test_df_copy[columns_to_clean].applymap(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d66861ca-6a66-456e-9410-60fde84c39c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>description</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rbi revise definition politically exposed pers...</td>\n",
       "      <td>central bank also asked chairperson chief exec...</td>\n",
       "      <td>reserve bank india rbi changed definition poli...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ndtv q2 net profit fall 57 4 r 5 55 crore impa...</td>\n",
       "      <td>ndtv consolidated revenue operation r 95 55 cr...</td>\n",
       "      <td>broadcaster new delhi television ltd monday re...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>akasa air well capitalised grow much faster ce...</td>\n",
       "      <td>initial share sale open public subscription ju...</td>\n",
       "      <td>homegrown server maker netweb technology india...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>india current account deficit decline sharply ...</td>\n",
       "      <td>current account deficit cad 3 8 per cent gdp u...</td>\n",
       "      <td>india current account deficit declined sharply...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>state borrowing cost soar 7 68 highest far fiscal</td>\n",
       "      <td>price shot reflecting overall higher risk aver...</td>\n",
       "      <td>state forced pay nose weekly auction debt tues...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           headlines  \\\n",
       "0  rbi revise definition politically exposed pers...   \n",
       "1  ndtv q2 net profit fall 57 4 r 5 55 crore impa...   \n",
       "2  akasa air well capitalised grow much faster ce...   \n",
       "3  india current account deficit decline sharply ...   \n",
       "4  state borrowing cost soar 7 68 highest far fiscal   \n",
       "\n",
       "                                         description  \\\n",
       "0  central bank also asked chairperson chief exec...   \n",
       "1  ndtv consolidated revenue operation r 95 55 cr...   \n",
       "2  initial share sale open public subscription ju...   \n",
       "3  current account deficit cad 3 8 per cent gdp u...   \n",
       "4  price shot reflecting overall higher risk aver...   \n",
       "\n",
       "                                             content  category  \n",
       "0  reserve bank india rbi changed definition poli...  business  \n",
       "1  broadcaster new delhi television ltd monday re...  business  \n",
       "2  homegrown server maker netweb technology india...  business  \n",
       "3  india current account deficit declined sharply...  business  \n",
       "4  state forced pay nose weekly auction debt tues...  business  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the firs five observations to check if the datacleaning function worked\n",
    "train_df_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358b9040-56cc-4d8a-8cf0-9b67b0220cbd",
   "metadata": {},
   "source": [
    "By applying these data cleaning techniques, we aim to build a high-quality dataset that improves the reliability of our classification model, ultimately leading to more accurate predictions of news categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e2d6f6-d7dd-4693-b51c-be10ef672ce5",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"six\"></a>\n",
    "## Exploratory Data Analysis (EDA)\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac0ec12-03d1-41e7-81b1-9da8da2d94af",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis (EDA) is a crucial step in our news classification project, allowing us to better understand the dataset's characteristics and distributions. In this phase, we will focus firstly on visualizing the distribution of news categories to identify any imbalances. This initial visualization will provide insights into how the categories are spread across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7cd69b0a-9b4d-4a45-946a-25f28df846e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the distribution of the categories\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(data=train_df_copy, x='category')\n",
    "plt.title('Distribution of News Categories\\n')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60880dad-e47d-45e6-8c70-1fed982ca29f",
   "metadata": {},
   "source": [
    "From the visualisation, we can clearly note some imbalances in the spread of our news categories. This can lead to a biased models that perform well on majority classes but poorly on minority classes. A balanced dataset generally helps the model generalize better, leading to improved performance in real-world applications where class distributions may vary.\n",
    "\n",
    "Let us now numerically verify the counts for each category. Understanding the extent of this imbalance is essential for implementing effective solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bf619a6-5de3-49b3-9517-41719d76841d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "education        1520\n",
       "technology       1280\n",
       "business         1120\n",
       "entertainment     960\n",
       "sports            640\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying the count of each category\n",
    "train_df_copy['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e172b679-361b-413f-817a-efcc496cb9ae",
   "metadata": {},
   "source": [
    "The above further verifies the imbalance of the categories in our training dataset. \n",
    "\n",
    "The correct approach would be to  use Synthetic Data Generation which uses techniques like SMOTE (Synthetic Minority Over-sampling Technique) to create synthetic examples for minority classes. However, after running a SMOTE techinque. we noted that it negatively affected the performance of the models. As a result, we have elected to leave the class imblanace as is, and use the F1-score for evaluation metrics. The F1-score is a better measure as it gives a balance between precision and recall, moreso in the wake of uneven class distribution.\r\n",
    "It is important that we vectorize our data. Vectorizing the data serves to convert unstructured text into structured numerical data that machine learning models can process. It also captures the significance of words and their relationships in the context of different categories using methods like TF-IDF or Bag-of-Words. This project will use the TD-IDF method.\n",
    "\n",
    "Once vectorised, we will then split our training data using the 80-20 split. For this project, we will train and validate the top 3 models on the train.csv data. And then select the best performing model at the end to validate on the test.csv data. This will ensure that that model can be tested on unseen data to evaluate its performance.ique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53e4b85f-975a-4721-8a6a-835e61c432b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.combine import SMOTEENN \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# preparing our features and target from the training and testing datasets\n",
    "X_train = train_df_copy[['headlines', 'description', 'content']]  # feature train column\n",
    "y_train = train_df_copy['category']  # target train column\n",
    "X_test = test_df_copy[['headlines', 'description', 'content']]  # feature column for testing\n",
    "y_test = test_df_copy['category']  # target column for testing\n",
    "\n",
    "# splitting the training dataset into train and test data using a 20/80 split\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train,  # Features\n",
    "    y_train,  # Target\n",
    "    test_size=0.2,  # 20% for validation, 80% for training\n",
    "    random_state=42,  # For reproducibility\n",
    "    stratify=train_df_copy['category']  # Ensures the split maintains the class distribution\n",
    ")\n",
    "\n",
    "# vectorising the training data\n",
    "vectorizer = TfidfVectorizer() \n",
    "# fitting and transforming the training data\n",
    "X_train_vec = vectorizer.fit_transform(X_train_split['headlines'] + ' ' + X_train_split['description'] + ' ' + X_train_split['content'])\n",
    "\n",
    "# transforming the validation data \n",
    "X_val_vec = vectorizer.transform(X_val_split['headlines'] + ' ' + X_val_split['description'] + ' ' + X_val_split['content'])\n",
    "\n",
    "# applying SMOTE to the training data\n",
    "#smote = SMOTE(random_state=42)\n",
    "#X_resampled, y_resampled = smote.fit_resample(X_train_vec, y_train)\n",
    "\n",
    "#combining SMOTE with random sampling on training data\n",
    "#smote_enn = SMOTEENN(random_state=42)\n",
    "#X_resampled, y_resampled = smote_enn.fit_resample(X_train_vec, y_train)\n",
    "\n",
    "# Check the new class distribution\n",
    "#print(pd.Series(y_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95e6f74-0371-4dc4-bda2-4a5b5ce768a4",
   "metadata": {},
   "source": [
    "We will now store the vectorised model. This will ensure consistency when applying our Streamlit app. When we vectorised our data, we created a specific mapping of text features. Saving this model will ensure we continue to use the same instance in our Streamlit app when we apply it to our project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5679a23f-1e85-4607-ba56-c7ee687aa3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving vectoriser to be used for streamlit app later\n",
    "import joblib  # For loading the model\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d11d1b7-00a9-4866-9415-93b42c03aa82",
   "metadata": {},
   "source": [
    "Furthermore, we will create word clouds for each category to visualize the most frequently occurring terms. By defining a function to generate these word clouds, we can gain additional insights into the key topics and themes present within each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dee8a402-dc77-44ba-a72d-ed265561c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def generate_word_clouds(df, category_column='category', text_columns=['headlines', 'description', 'content']):\n",
    "    categories = df[category_column].unique()  # Getting unique categories\n",
    "\n",
    "    # Ensuring there are no more than 5 categories\n",
    "    if len(categories) > 5:\n",
    "        raise ValueError(\"This function supports up to 5 categories only.\")\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))  # Setting figure size\n",
    "    \n",
    "    for i, category in enumerate(categories):\n",
    "        plt.subplot(3, 2, i + 1)  # Creating a subplot for each category (adjust grid size)\n",
    "        \n",
    "        # Combine text from all specified columns for the current category\n",
    "        text = ' '.join(df[df[category_column] == category][text_columns].apply(lambda x: ' '.join(x), axis=1))\n",
    "        \n",
    "        # Generate the word cloud for the combined text\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "        \n",
    "        # Display the word cloud\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')  # Turn off axis\n",
    "        plt.title(category)  # Set the title to the category\n",
    "\n",
    "    plt.tight_layout()  # Adjust subplots to fit into the figure area\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcd09974-9978-4b96-9c5c-a0aaaf5e471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_word_clouds(train_df_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24271379-4a9f-4070-9384-1360beab6ebe",
   "metadata": {},
   "source": [
    "The EDA phase has provided a comprehensive overview of our dataset, helping us identify class imbalances and the key characteristics of each news category. By visualizing category distributions, verifying counts, and generating word clouds, we have laid a solid foundation for subsequent modeling efforts, ultimately enhancing the performance of our classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e93e51-705e-456f-859d-dd735febc429",
   "metadata": {},
   "source": [
    "----\n",
    "<a id=\"seven\"></a>\n",
    "## Modeling\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7962368-c2dc-48f0-a0a8-d70308760ec3",
   "metadata": {},
   "source": [
    "The modeling stage is a critical phase in a data science project where the goal is to build a machine learning model (or several models) that can make accurate predictions or insights based on the data.\n",
    "\n",
    "Given that the category column is a categorical variable, it is important that we label- encode it. Label encoding is a technique used in machine learning to convert categorical labels (text values) into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "295e4635-1700-4f0d-904a-c635d0587b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Instantiate the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the target labels to numeric values\n",
    "y_train_split_encoded = label_encoder.fit_transform(y_train_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07d6383-4e7b-40b8-b615-ed79ffc70d31",
   "metadata": {},
   "source": [
    "Training multiple classification models helps us selecting the best performing model, improving generalization, handling different types of data, and increasing robustness. So next, we will train five classification models and evaluate them using cross-validation method. Cross-validation is a technique used to evaluate the performance of a machine learning model by splitting the dataset into multiple subsets (or folds) and training/testing the model on different combinations of these subsets. This will help improve the generalizability of a model, help in detecting overfitting, and provide a more accurate estimate of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21b667c1-aa29-4893-a632-d0d5d0eee012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training multiple classification models\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# instantiating classifiers\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(class_weight='balanced'),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'SVM': SVC(class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(class_weight='balanced'),\n",
    "    'XGBoost': xgb.XGBClassifier(class_weight='balanced')\n",
    "}\n",
    "\n",
    "# Train models and store them\n",
    "model_results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_vec, y_train_split_encoded)\n",
    "    model_results[name] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "790a4a97-3382-4dc9-a756-c125a99e1df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: Cross-validation Accuracy: 0.9789\n",
      "Naive Bayes: Cross-validation Accuracy: 0.9644\n",
      "SVM: Cross-validation Accuracy: 0.9769\n",
      "Random Forest: Cross-validation Accuracy: 0.9601\n",
      "XGBoost: Cross-validation Accuracy: 0.9633\n"
     ]
    }
   ],
   "source": [
    "# model evaluation using cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform cross-validation on each model\n",
    "cv_results = {}\n",
    "for name, model in model_results.items():\n",
    "    cv_scores = cross_val_score(model, X_train_vec, y_train_split_encoded, cv=5, scoring='accuracy')\n",
    "    cv_results[name] = cv_scores.mean()\n",
    "\n",
    "# Display cross-validation results\n",
    "for name, score in cv_results.items():\n",
    "    print(f'{name}: Cross-validation Accuracy: {score:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de275418-26eb-49e9-ae36-9094d6cc94ac",
   "metadata": {},
   "source": [
    "Observation:\n",
    "\n",
    "From the above results, we note that Logistic Regression (0.9789) has the highest cross-validation accuracy among all the models, suggesting it is the most effective model for this particular problem. The slight edge in performance could be due to the simplicity and efficiency of this model in capturing linear relationships in the data.\n",
    "\n",
    "SVM (0.9769) and Naive Bayes (0.9644) also perform well but slightly behind logistic regression.The close performance between SVM and Logistic Regression suggests that the data might be relatively simple, as both models are linear classifiers (though SVM can use kernels for non-linear data).\n",
    "Naive Bayes, though slightly less accurate, still performs well. It may indicate that feature independence assumptions (used in Naive Bayes) hold reasonably true for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9399a4ba-7c1f-4012-a4f2-3e9ca264832e",
   "metadata": {},
   "source": [
    "For the next step, we will fine tune the performance of these top 3 performing models in order to enhance their performance. We will use the GridSearch approach to do this. Grid Search Cross-Validation (GridSearchCV) is a technique used for hyperparameter tuning in machine learning models. The purpose is to systematically explore a specified set of hyperparameters for a given model to find the combination that yields the best performance. This will help us in ensuring that we get the best possible configuration for our model. In our news article classification project, this leads to better accuracy, model reliability, and generalization, which are essential for creating a robust and effective classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "863a1b5b-e787-4815-9ff9-20cc21684bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Logistic Regression: {'C': 10}\n",
      "Best Parameters for Naive Bayes: {'alpha': 0.1}\n",
      "Best Parameters for SVM: {'C': 10, 'gamma': 1, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning using GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.1, 1, 10]  # Regularization strength\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],  # Regularization parameter\n",
    "        'kernel': ['linear', 'rbf'],  # Linear or Radial basis function kernel\n",
    "        'gamma': [1, 0.1, 0.01, 0.001],  # Kernel coefficient for 'rbf' (if kernel='rbf')\n",
    "    },\n",
    "    'Naive Bayes': {\n",
    "        'alpha': [0.1, 0.5, 1.0, 2.0]  # Smoothing parameter (Laplace smoothing)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for each model\n",
    "best_models = {}\n",
    "for name, model in model_results.items():\n",
    "    if name in param_grids:\n",
    "        grid_search = GridSearchCV(model, param_grids[name], cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train_vec, y_train_split_encoded)\n",
    "        best_models[name] = grid_search.best_estimator_\n",
    "        print(f\"Best Parameters for {name}: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb452a4-a15b-4168-b347-1912b68d88eb",
   "metadata": {},
   "source": [
    "Observation: \n",
    "\n",
    "Both Logistic Regression and SVM have C = 10, indicating that the optimal models for this dataset benefit from less regularization and are more flexible in fitting the data. Naive Bayes is optimal with a small alpha = 0.1, indicating that the features likely have strong predictive power, and the model does not need heavy adjustments for unseen data. This likely implies that the dataset has clean and informative features. The best parameters across these models indicate that regularization, feature separability, and model flexibility are key factors in achieving optimal performance for this news article classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109ca010-9742-45e7-b6dd-fa3631f9544e",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"eight\"></a>\n",
    "## Evaluation \n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fd0ad8-4b0b-45f6-a474-e2dc22844749",
   "metadata": {},
   "source": [
    "The evaluation and validation stage of a data science project is crucial for assessing the performance and reliability of the model. It involves using various metrics to ensure that the model generalizes well to unseen data and performs effectively in real-world scenarios. In this phase, we evaluate how well the model predicts outcomes based on a set of predefined criteria of accuracy, F1-scores and weighted avg. \n",
    "\n",
    "The approach we have taken in this notebook is to evaluate the three models using the the classification report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63621b8d-1099-464c-926f-3bdb46ca9824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the target labels to numeric values\n",
    "y_val_split_encoded = label_encoder.fit_transform(y_val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12ddebb5-6c30-463b-8315-52932dc6316b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97       224\n",
      "           1       1.00      0.99      0.99       304\n",
      "           2       1.00      1.00      1.00       192\n",
      "           3       0.98      0.98      0.98       128\n",
      "           4       0.98      0.98      0.98       256\n",
      "\n",
      "    accuracy                           0.99      1104\n",
      "   macro avg       0.99      0.99      0.99      1104\n",
      "weighted avg       0.99      0.99      0.99      1104\n",
      "\n",
      "Accuracy: 0.9864\n",
      "Classification Report for Naive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98       224\n",
      "           1       1.00      1.00      1.00       304\n",
      "           2       0.99      1.00      0.99       192\n",
      "           3       0.98      0.97      0.98       128\n",
      "           4       0.98      0.98      0.98       256\n",
      "\n",
      "    accuracy                           0.99      1104\n",
      "   macro avg       0.99      0.98      0.99      1104\n",
      "weighted avg       0.99      0.99      0.99      1104\n",
      "\n",
      "Accuracy: 0.9864\n",
      "Classification Report for SVM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98       224\n",
      "           1       0.99      1.00      1.00       304\n",
      "           2       1.00      1.00      1.00       192\n",
      "           3       0.98      0.98      0.98       128\n",
      "           4       0.97      0.97      0.97       256\n",
      "\n",
      "    accuracy                           0.98      1104\n",
      "   macro avg       0.99      0.98      0.98      1104\n",
      "weighted avg       0.98      0.98      0.98      1104\n",
      "\n",
      "Accuracy: 0.9846\n"
     ]
    }
   ],
   "source": [
    "# evaluating the best model\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Evaluate each best model on the test set\n",
    "for name, model in best_models.items():\n",
    "    y_pred = model.predict(X_val_vec)\n",
    "    print(f'Classification Report for {name}:')\n",
    "    print(classification_report(y_val_split_encoded, y_pred))\n",
    "    print(f'Accuracy: {accuracy_score(y_val_split_encoded, y_pred):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65703ec6-dc30-4f76-8365-65829f0a6ee3",
   "metadata": {},
   "source": [
    "Observation\n",
    "\n",
    "**Accuracy Comparison:**\n",
    "\n",
    "Logistic Regression and Naive Bayes have the highest accuracy (both 0.9864), while SVM has slightly lower accuracy (0.9846). These differences in accuracy are small, but Logistic Regression and Naive Bayes are tied for the best.\n",
    "\n",
    "**Macro and Weighted Averages:**\n",
    "\n",
    "Both Logistic Regression and Naive Bayes have higher weighted average f1-scores (0.99 vs 0.98 for SVM), which means they handle class imbalances better and perform consistently across all classes.\n",
    "\n",
    "**Precision, Recall, and F1-Score for each class:**\n",
    "\n",
    "Logistic Regression and Naive Bayes perform almost identically across all classes, with very high precision, recall, and f1-scores for each class. While SVM is strong overall, it has slightly lower recall and precision for class 0 (label 'business') and class 4 (label 'technology'). This suggests that SVM might miss or misclassify a few instances of these categories compared to Logistic Regression or Naive Bayes.\n",
    "\n",
    "Key Insights:\n",
    "Logistic Regression and Naive Bayes have identical results, both with a slight edge over SVM in terms of handling all classes equally well. Given that these models are tied in performance metrics, either one of them could be considered the best.\n",
    "\n",
    "Logistic Regression is often preferred when you are dealing with text data in multi-class classification problems. It’s fast and provides well-calibrated probabilities. If interpretability is important, Logistic Regression also tends to provide clearer insights into the importance of each feature.\n",
    "\n",
    "Both models will provide excellent performance, but Logistic Regression is likely the preferred choice in this case due to its overall balance between accuracy and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67319a51-c0c0-465c-989c-c8fcd904e45c",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"nine\"></a>\n",
    "## Final Model\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1ef18f-5dba-4228-93e1-79ecbee772af",
   "metadata": {},
   "source": [
    "Now that we concluded on the Logistic Regression model being the model that provides superior performance for our news article category prediction, we will retrain the model on the full train.csv dataset, and then predict and validate the model performance on unseen data from the train.csv dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3a87754-0967-471c-b621-a05cb0d5b050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97       400\n",
      "           1       1.00      0.98      0.99       400\n",
      "           2       0.99      0.99      0.99       400\n",
      "           3       0.99      0.99      0.99       400\n",
      "           4       0.94      0.97      0.96       400\n",
      "\n",
      "    accuracy                           0.98      2000\n",
      "   macro avg       0.98      0.98      0.98      2000\n",
      "weighted avg       0.98      0.98      0.98      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Preprocess and vectorize the training and test data\n",
    "vectorizer = TfidfVectorizer()  # Example: Bi-grams and Tri-grams\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_vec = vectorizer.fit_transform(train_df_copy['headlines'] + ' ' + train_df_copy['description'] + ' ' + train_df_copy['content'])\n",
    "\n",
    "# Transform the test data using the same vectorizer (do NOT fit it again)\n",
    "X_test_vec = vectorizer.transform(test_df_copy['headlines'] + ' ' + test_df_copy['description'] + ' ' + test_df_copy['content'])\n",
    "\n",
    "# 2. Encode the target labels for both training and test sets\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the training target labels\n",
    "y_train_encoded = label_encoder.fit_transform(train_df_copy['category'])\n",
    "\n",
    "# Transform the test target labels (use the same encoder, do not fit again)\n",
    "y_test_encoded = label_encoder.transform(test_df_copy['category'])\n",
    "\n",
    "# 3. Train Logistic Regression on the entire training dataset\n",
    "logreg = LogisticRegression(class_weight='balanced')  # Use class_weight='balanced' if you have class imbalance\n",
    "logreg.fit(X_train_vec, y_train_encoded)\n",
    "\n",
    "# 4. Make predictions on the test dataset\n",
    "y_pred = logreg.predict(X_test_vec)\n",
    "\n",
    "# 5. Optionally, convert the numeric predictions back to original labels\n",
    "y_pred_original = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "# 6. Evaluate model performance on the test set\n",
    "print(classification_report(y_test_encoded, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8084495e-78c2-4089-829e-2c6fa3e1c276",
   "metadata": {},
   "source": [
    "The model achieved an overall accuracy of 98%, which indicates that it performs exceptionally well across all classes. This means that the model correctly classifies 98% of the total instances in the dataset, suggesting a high level of reliability and robustness in its predictions.\n",
    "\n",
    "Precision, Recall, and F1-score are consistently high across all classes, ranging from 0.94 to 1.00. This suggests that the model is well-calibrated and performs reliably across all categories, with no class being underrepresented or overrepresented. Specifically:\n",
    "* Class 1 has the highest precision (1.00), meaning it has perfect accuracy in classifying instances of this category.\n",
    "* Class 4 has the lowest precision (0.94), but still performs well, indicating some room for improvement in correctly identifying this category.\n",
    "\n",
    "Recall (sensitivity) is high for all classes, with the lowest value being 0.96 for class 0. This means that the model successfully identifies most of the actual instances of each class.\n",
    "\n",
    "he model exhibits excellent performance with high accuracy, balanced precision and recall across all classes, and strong F1-scores. However, slight improvement can be made in class 4, where precision is slightly lower (0.94), but the overall model is very robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c03750d-a9a5-45b9-ae09-8910ce437129",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"ten\"></a>\n",
    "## Streamlit Deployment\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c2b451-848d-4cb5-be7c-f9fee6970c07",
   "metadata": {},
   "source": [
    "This section of the project focuses on creating an interactive web application that allows users to interact with the trained model in a user-friendly interface. Streamlit is a powerful framework that enables quick and easy development of data science and machine learning apps. By deploying the model with Streamlit, we provide a simple, visual way for users to input new data (such as news articles), view predictions, and explore model results. This section will guide you through the process of converting the trained model into a deployable web app, allowing for real-time predictions and ensuring that the model is accessible and easy to use for both technical and non-technical users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fda90e28-7509-4732-88b4-bdf49fa4eacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, Vectorizer, and Label Encoder saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# saving the model, vectorizer, and label encoder using joblib\n",
    "joblib.dump(logreg, 'final_logreg_model.pkl')  # Save the trained Logistic Regression model\n",
    "joblib.dump(vectorizer, 'final_vectorizer.pkl')  # Save the fitted TfidfVectorizer\n",
    "joblib.dump(label_encoder, 'final_label_encoder.pkl')  # Save the LabelEncoder\n",
    "\n",
    "print(\"Model, Vectorizer, and Label Encoder saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a8ffec6-354f-4b15-9f4c-fa450ab10715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Category: technology\n"
     ]
    }
   ],
   "source": [
    "# loading the saved model, vectorizer, and label encoder\n",
    "logreg = joblib.load('final_logreg_model.pkl')\n",
    "vectorizer = joblib.load('final_vectorizer.pkl')\n",
    "label_encoder = joblib.load('final_label_encoder.pkl')\n",
    "\n",
    "# Example text to classify\n",
    "text = \"Example news article content here.\"\n",
    "\n",
    "# Transform the text using the same vectorizer\n",
    "text_vectorized = vectorizer.transform([text])\n",
    "\n",
    "# Predict the category using the loaded model\n",
    "y_pred = logreg.predict(text_vectorized)\n",
    "\n",
    "# Convert the numeric prediction back to the original label\n",
    "y_pred_original = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "print(f\"Predicted Category: {y_pred_original[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10c8a843-befe-4ce3-bd42-3702cdd2f139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 16:29:07.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-20 16:29:07.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-20 16:29:07.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-20 16:29:07.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-20 16:29:07.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-20 16:29:07.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-20 16:29:07.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-20 16:29:07.718 Session state does not function when running a script without `streamlit run`\n",
      "2024-10-20 16:29:07.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-20 16:29:07.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-20 16:29:07.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-20 16:29:07.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-20 16:29:07.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-10-20 16:29:07.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# running the streamlit app code\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "# loading the saved model and vectorizer\n",
    "model = joblib.load('final_logreg_model.pkl')\n",
    "vectorizer = joblib.load('final_vectorizer.pkl')\n",
    "\n",
    "# Function to predict the category\n",
    "def predict_category(text):\n",
    "    # Transform the input text using the same vectorizer\n",
    "    text_vectorized = vectorizer.transform([text])\n",
    "    \n",
    "    # Get the prediction from the model\n",
    "    prediction = model.predict(text_vectorized)\n",
    "    \n",
    "    # Return the predicted class label\n",
    "    return prediction[0]\n",
    "\n",
    "# Streamlit app layout\n",
    "st.title('News Classification Application')\n",
    "st.subheader('Enter a news article to classify its category')\n",
    "\n",
    "# Input text box\n",
    "article_text = st.text_area('Enter article text:', height=300)\n",
    "\n",
    "# When the user clicks the classify button\n",
    "if st.button('Classify'):\n",
    "    if article_text.strip() != \"\":\n",
    "        category = predict_category(article_text)\n",
    "        st.write(f'Predicted Category: {category}')\n",
    "    else:\n",
    "        st.write(\"Please enter some text to classify.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03782155-8033-452e-a76f-45ab478084cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app.py file has been saved successfully!\n"
     ]
    }
   ],
   "source": [
    "app_code = \"\"\"\n",
    "import streamlit as st\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Load the saved model, vectorizer, and label encoder\n",
    "logreg = joblib.load('final_logreg_model.pkl')\n",
    "vectorizer = joblib.load('final_vectorizer.pkl')\n",
    "label_encoder = joblib.load('final_label_encoder.pkl')\n",
    "\n",
    "# Function to predict the category\n",
    "def predict_category(text):\n",
    "    # Transform the input text using the same vectorizer\n",
    "    text_vectorized = vectorizer.transform([text])\n",
    "    \n",
    "    # Get the prediction from the model\n",
    "    prediction = logreg.predict(text_vectorized)\n",
    "    \n",
    "    # Return the predicted class label\n",
    "    return label_encoder.inverse_transform(prediction)[0]\n",
    "\n",
    "# Streamlit app layout\n",
    "st.title('News Classification Application')\n",
    "st.subheader('Enter a news article to classify its category')\n",
    "\n",
    "# Input text box\n",
    "article_text = st.text_area('Enter article text:', height=300)\n",
    "\n",
    "# When the user clicks the classify button\n",
    "if st.button('Classify'):\n",
    "    if article_text.strip() != \"\":\n",
    "        category = predict_category(article_text)\n",
    "        st.write(f'Predicted Category: {category}')\n",
    "    else:\n",
    "        st.write(\"Please enter some text to classify.\")\n",
    "\"\"\"\n",
    "\n",
    "# Save the code to a file named 'app.py'\n",
    "with open(\"app.py\", \"w\") as f:\n",
    "    f.write(app_code)\n",
    "\n",
    "print(\"app.py file has been saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9752ec1-c699-451e-a54c-c85af51cc849",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"eleven\"></a>\n",
    "## Conclusion and Future Work\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f41ee1-8e27-461d-ae9f-50ab5fed81fc",
   "metadata": {},
   "source": [
    "The News Article Category Prediction project successfully developed a machine learning model capable of classifying news articles into predefined categories. Through a series of well-defined steps, the project focused on data preprocessing, model selection, and evaluation to ensure the final model provides accurate and reliable predictions. Key stages included cleaning and transforming the dataset, exploring and addressing class imbalance, and leveraging multiple classification algorithms to find the best-performing model. After rigorous testing and hyperparameter tuning using GridSearchCV, Logistic Regression emerged as the most effective model, outperforming others based on accuracy and F1-score metrics.\n",
    "\n",
    "Additionally, the project culminated in the development of an interactive web application using Streamlit, enabling real-time predictions of news article categories. This makes the model accessible to users, allowing them to input new articles and receive predictions in an intuitive interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a873b240-561b-443b-9bf4-5f148b687f75",
   "metadata": {},
   "source": [
    "#### Considerations for Future Work\n",
    "\n",
    "While the current model provides strong performance, there are several areas for potential improvement and future exploration:\n",
    "\n",
    "1. Handling Class Imbalance:\n",
    "Although the project identified a class imbalance, the decision to avoid SMOTE due to performance issues should be revisited. Exploring other techniques such as class weighting or undersampling may yield better results without affecting model performance.\n",
    "\n",
    "2. Text Feature Engineering:\n",
    "Further improvements could be made to text feature extraction, such as exploring word embeddings (e.g., Word2Vec, GloVe) or contextual embeddings like BERT for a more nuanced understanding of the text data.\n",
    "\n",
    "3. Real-Time Model Updates:\n",
    "As the news domain is dynamic and constantly evolving, regular model updates could be implemented to ensure the classifier remains accurate and relevant. Automating the retraining process with fresh data would help maintain model performance over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc8d11b-2f2c-4895-92a9-5e091a3595f2",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"twelve\"></a>\n",
    "## References\n",
    "<a href=#cont>Back to Table of Contents</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1065e9-9605-484b-a10f-7e63d792f61a",
   "metadata": {},
   "source": [
    "[1] Rao, S., Sudarshan, K. and Abhishek (2020) 'News Article Category Predictor', Department of Computer Science and Engineering, Srinivas Institute of Technology, Valachil, India.\n",
    "\n",
    "[2] Tong, S. and Koller, D. (2000) 'Support vector machine active learning with applications to text classification', in Langley, P. (ed.) Proceedings ICML-00, 17th International Conference on Machine Learning, pp. 999–1006.\n",
    "\n",
    "[3] McCallum, A. and Nigam, K. (1998) 'A comparison of event models for naive Bayes text classification', in AAAI/ICML-98 Workshop on Learning for Text Categorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b60dd6-5d26-4dfc-8642-02086b23beaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
